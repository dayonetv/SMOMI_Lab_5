# Исследование влияния техники Fine Tuning на процесс обучения нейронной сети на примере решения задачи классификации Oregon Wildlife  
В данной лабораторной работе будут использоваться следующие техники аугментации данных с оптимальными параметрами:  
+ Манипуляции с яркостью и контрастом (contrast_factor = 2, delta = 0.3 - contrast_factor - множитель для регулировки контраста, delta - число добавляемое к значениям пикселей для изменения яркости)  
+ Поворот изображения на случайный угол (factor = 0.02 - параметр, определяющий диапазон из которого будет случайно выбран угол поворота)  
+ Использование случайной части изображения (height = 235, width = 235 - высота (height) и ширина (width), которые будут у выходного изображения после предварительного масштабирования)   
+ Добавление случайного шума (stddev = 0.6 - значение среднеквадратичного отклонения добавляемого шума)  
***
В данной лабораторной работе для решения задачи классификации изображений Oregon Wildlife использовалась нейронная сеть EfficientNet-B0, причем данная нейронная сеть будет иметь уже предобученные веса на базе изображений ImageNet. В качестве политики изменения темпа обучения была использована политика экспоненциального затухания с оптимальными параметрами initial_lrate = 0.01 и k = 0.3 (initial_lrate - начальный темп обучения, k - коэффициент наклона экспоненциальной кривой).  
```
def build_model():  
  inputs = tf.keras.Input(shape=(RESIZE_TO, RESIZE_TO, 3))  
  x = tf.keras.layers.GaussianNoise(stddev = 0.6)(inputs)  
  x = tf.keras.layers.experimental.preprocessing.RandomCrop(224,224)(x)  
  x = tf.keras.layers.experimental.preprocessing.RandomRotation(factor = 0.02)(x)  
  model = EfficientNetB0(include_top=False, input_tensor = x, pooling = 'avg', weights='imagenet')  
  model.trainable = False  
  x = tf.keras.layers.Flatten()(model.output)  
  outputs = tf.keras.layers.Dense(NUM_CLASSES, activation = tf.keras.activations.softmax)(x)  
  return tf.keras.Model(inputs=inputs, outputs=outputs)  
```
Обучение нейронной сети будет проводиться в два этапа:  
+ Первый этап - заморозить все слои нейронной сети EfficientNet-B0 (model.trainable = False) и обучить только верхние слои (классификатор). - Transfer Learning    
+ Второй этап - разморозить несколько слоев сети EfficientNet-B0, а затем дообучить ее с меньшим темпом обучения. - Fine Tuning   
Таким образом нейронная сеть будет лучше выделять какие-то характерные признаки тех объектов, которые входят в используемый датасет Oregon Wildlife.  
Функция для разморозки верхних слоев нейронной сети EfficientNet-B0:  
```
def unfreeze_model(model):  
  for layer in model.layers[-20:]:  
    if not isinstance(layer, tf.keras.layers.BatchNormalization):  
      layer.trainable = True  
```
С помощью данной функции будут размораживаться последние 20 слоев нейронной сети EfficientNet-B0, кроме слоев ```BatchNormalization```.  
## С использованием техники обучения Transfer Learning и оптимальной политики изменения темпа обучения обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Oregon Wildlife с совместным использованием техник аугментации данных с оптимальными параметрами:  
#### В результате получили следующие графики:  
+ График метрики точности для предобученной нейронной сети EfficientNet-B0 для тренировочного и валидационного наборов данных:  

![изображение](https://user-images.githubusercontent.com/59259102/113625713-f0c35e00-9669-11eb-846d-945227d7bade.png)   
  
<img src="./epoch_categorical_accuracy_all_in_one.svg">

* График функции потерь для предобученной нейронной сети EfficientNet-B0 для тренировочного и валидационного наборов данных:  

![изображение](https://user-images.githubusercontent.com/59259102/113625617-cd001800-9669-11eb-8c48-0b65fefc77fd.png)  
 

<img src="./epoch_loss_all_in_one.svg">


* Анализ полученных результатов  
Исходя из полученных результатов можно отметить следующее (для валидационного набора данных):  
  1. Cкорость схождения алгоритма составляет около 20-22 эпох.  
  2. Метрика точности в конце обучения составляет 89.22 процента (наибольшая точность достигается на 21 эпохе и составляет 89.3 процента).  
  3. Значение потерь в конце обучения составляет 0.278 и достигается уже на 21 эпохе.   
***
### Использование техники обучения Fine Tuning для дополнительного обучения нейронной сети EfficientNet-B0 предварительно обученной в пункте выше  
В данной части лабораторной работы нейронная сеть будет обучаться в два этапа:
+ Первый этап - заморозить все слои нейронной сети EfficientNet-B0 (model.trainable = False) и обучить только верхние слои (классификатор). - Transfer Learning    
+ Второй этап - разморозить несколько слоев сети EfficientNet-B0, а затем дообучить ее с меньшим темпом обучения. - Fine Tuning   
***
Так как первый этап был выполнен выше, то для него будут использоваться не все 50 эпох, а только 25, так как скорость схождения алгоритма составляет около 20-22 эпох.  
Для второго этапа будет использоваться еще 20 эпох. Таким образом для всего обучения будет использоваться 45 эпох.  
Также для техники обучения Fine Tuning будут изменяться следующие параметры:  
+ Темп обучения  
+ Количество размораживаемых слоев  
### Подбор оптимальных параметров:  
Для нахождения оптимальных параметров, были проведены обучения с различными параметрами, такими как (lr - темп обучения, layers - количество размораживаемых слоев):
+ lr = 1e-10, layers = -20  
+ lr = 1e-8, layers = -20  
+ lr = 1e-6, layers = -20  
+ lr = 1e-4, layers = -20  
+ lr = 1e-6, layers = all  
+ lr = 1e-8, layers = -10  
#### В результате получили следующие графики (использование техники Fine Tuning с различными параметрами):    
+ График метрики точности для предобученной нейронной сети EfficientNet-B0 (использование техники Fine Tuning) для валидационного набора данных:  
  + До разморозки слоев (25 эпох):  

![изображение](https://user-images.githubusercontent.com/59259102/113636042-6f27fc00-967a-11eb-92f2-97973b82e4fc.png)  

<img src="./epoch_categorical_accuracy_before_unfreeze.svg">  

  + После разморозки слоев (20 эпох):  

![изображение](https://user-images.githubusercontent.com/59259102/113634871-40a92180-9678-11eb-9c61-aa332f66de34.png)  

<img src="./epoch_categorical_accuracy_unfreeze.svg">  

* График функции потерь для предобученной нейронной сети EfficientNet-B0 (использование техники Fine Tuning) для валидационного набора данных:  
  + До разморозки слоев (25 эпох):  

![изображение](https://user-images.githubusercontent.com/59259102/113636100-98e12300-967a-11eb-8ef2-7814c1793d07.png)  

<img src="./epoch_loss_before_unfreeze.svg">  

  + После разморозки слоев (20 эпох):  

![изображение](https://user-images.githubusercontent.com/59259102/113634916-50c10100-9678-11eb-8b2f-b1dd4ba72eba.png)  

<img src="./epoch_loss_unfreeze.svg">  


* Анализ полученных результатов  
Исходя из полученных результатов можно отметить следующее:  
В качестве оптимальных параметров для техники Fine Tuning можно использовать: lr = 1e-8, layers = -10 (lr - темп обучения, layers - количество размораживаемых слоев).
  + По сравнению с другими использовавшимися комбинациями:  
    1. Единственная комбинация, которая привела, хоть и к небольшому, увеличению тоности по сравнению с политикой без fine tuning (89.33 процента)  
    2. Одно из наименьших значений потерь в конце обучения (значение потерь 0.2769)  
  + По сравнению с политикой без техники Fine Tuning:  
    1. Увеличение точности на 16 эпохе после разморозки части слоев, по сравнению с политикой без fine tuning (увеличение точности на 0.08 процента)  
    2. Не уменьшилось значение потерь, а после разморозки части слоев даже начало не намного увеличиваться, по сравнению с политикой fine tuning (увеличение потерь на 0.0004)  
  + Таким образом, использование техники Fine Tuning данных особо не привело к улучшению качества обученной модели по точности при оптимально подобранных параметрах, в сравнении с политикой без техники Fine Tuning.  
