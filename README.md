# Исследование влияния техники Fine Tuning на процесс обучения нейронной сети на примере решения задачи классификации Oregon Wildlife  
В данной лабораторной работе будут использоваться следующие техники аугментации данных с оптимальными параметрами:  
+ Манипуляции с яркостью и контрастом (contrast_factor = 2, delta = 0.3 - contrast_factor - множитель для регулировки контраста, delta - число добавляемое к значениям пикселей для изменения яркости)  
+ Поворот изображения на случайный угол (factor = 0.02 - параметр, определяющий диапазон из которого будет случайно выбран угол поворота)  
+ Использование случайной части изображения (height = 235, width = 235 - высота (height) и ширина (width), которые будут у выходного изображения после предварительного масштабирования)   
+ Добавление случайного шума (stddev = 0.6 - значение среднеквадратичного отклонения добавляемого шума)  
***
В данной лабораторной работе для решения задачи классификации изображений Oregon Wildlife использовалась нейронная сеть EfficientNet-B0, причем данная нейронная сеть будет иметь уже предобученные веса на базе изображений ImageNet. В качестве политики изменения темпа обучения была использована политика экспоненциального затухания с оптимальными параметрами initial_lrate = 0.01 и k = 0.3 (initial_lrate - начальный темп обучения, k - коэффициент наклона экспоненциальной кривой).  
```
def build_model():  
  inputs = tf.keras.Input(shape=(RESIZE_TO, RESIZE_TO, 3))  
  x = tf.keras.layers.GaussianNoise(stddev = 0.6)(inputs)  
  x = tf.keras.layers.experimental.preprocessing.RandomCrop(224,224)(x)  
  x = tf.keras.layers.experimental.preprocessing.RandomRotation(factor = 0.02)(x)  
  model = EfficientNetB0(include_top=False, input_tensor = x, pooling = 'avg', weights='imagenet')  
  model.trainable = False  
  x = tf.keras.layers.Flatten()(model.output)  
  outputs = tf.keras.layers.Dense(NUM_CLASSES, activation = tf.keras.activations.softmax)(x)  
  return tf.keras.Model(inputs=inputs, outputs=outputs)  
```
Обучение нейронной сети будет проводиться в два этапа:  
+ Первый этап - заморозить все слои нейронной сети EfficientNet-B0 (model.trainable = False) и обучить только верхние слои (классификатор). - Transfer Learning    
+ Второй этоп - разморозить несколько верхних слоев сети EfficientNet-B0, а затем дообучить ее с меньшим темпом обучения. - Fine Tuning   
Таким образом нейронная сеть будет лучше выделять какие-то характерные признаки тех объектов, которые входят в используемый датасет Oregon Wildlife.  
Функция для разморозки верхних слоев нейронной сети EfficientNet-B0:  
```
def unfreeze_model(model):  
  for layer in model.layers[-20:]:  
    if not isinstance(layer, tf.keras.layers.BatchNormalization):  
      layer.trainable = True  
```
С помощью данной функции будут размораживаться последние 20 слоев нейронной сети EfficientNet-B0, кроме слоев ```BatchNormalization```.  
## С использованием техники обучения Transfer Learning и оптимальной политики изменения темпа обучения обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Oregon Wildlife с совместным использованием техник аугментации данных с оптимальными параметрами:  
#### В результате получили следующие графики:  
+ График метрики точности для предобученной нейронной сети EfficientNet-B0 для тренировочного и валидационного наборов данных:

![legend_accuracy_brigthness_contrast](https://user-images.githubusercontent.com/59259102/112815848-6b4c1680-9089-11eb-9ef2-a06840cf72f3.png)  

<img src="./epoch_categorical_accuracy_brigtness_contrast.svg">

* График функции потерь для предобученной нейронной сети EfficientNet-B0 для тренировочного и валидационного наборов данных:  

![legend_loss_brightness_contrast](https://user-images.githubusercontent.com/59259102/112822862-29bf6980-9091-11eb-8989-d7d83ce88fc8.png)  
 

<img src="./epoch_loss_brightness_contrast.svg">


* Анализ полученных результатов  
Исходя из полученных результатов можно отметить следующее (для валидационного набора данных):  
  1. Cкорость схождения алгоритма составляет около 20-22 эпох.  
  2. Метрика точности в конце обучения составляет 89.22 процента (наибольшая точность достигается на 21 эпохе и состовляет 82.3 процента).  
  3. Значение потерь в конце обучения составляет 0.278 и достигается уже на 21 эпохе.   
***
### Использование техники обучения Fine Tuning для дополнительного обучения нейронной сети EfficientNet-B0 предварительно обученной в пункте выше  
В данной части лабораторной работы нейронная сеть будет обучаться в два этапа:
+ Первый этап - заморозить все слои нейронной сети EfficientNet-B0 (model.trainable = False) и обучить только верхние слои (классификатор). - Transfer Learning    
+ Второй этоп - разморозить несколько верхних слоев сети EfficientNet-B0, а затем дообучить ее с меньшим темпом обучения. - Fine Tuning   
Так как первый этап был выполнен выше, то для него будут использоваться не все 50 эпох, а только 25, так как скорость схождения алгоритма составляет около 20-22 эпох.  
Для второго этапа будет использоваться еще 20 эпох. Таким образом для всего обучения будет использоваться 45 эпох.  
Также для техники обучения Fine Tuning будут изменяться следующие параметры:  
+ Темп обучения  
+ Количество размораживаемых слоев  
+ 
