# Исследование влияния техники Fine Tuning на процесс обучения нейронной сети на примере решения задачи классификации Oregon Wildlife  
В данной лабораторной работе будут использоваться следующие техники аугментации данных с оптимальными параметрами:  
+ Манипуляции с яркостью и контрастом (contrast_factor = 2, delta = 0.3 - contrast_factor - множитель для регулировки контраста, delta - число добавляемое к значениям пикселей для изменения яркости)  
+ Поворот изображения на случайный угол (factor = 0.02- параметр, определяющий диапазон из которого будет случайно выбран угол поворота)  
+ Использование случайной части изображения (height = 235, width = 235 - высота (height) и ширина (width), которые будут у выходного изображения после предварительного масштабирования)   
+ Добавление случайного шума (stddev = 0.6 - значение среднеквадратичного отклонения добавляемого шума)  
***
В данной лабораторной работе для решения задачи классификации изображений Oregon Wildlife использовалась нейронная сеть EfficientNet-B0, причем данная нейронная сеть будет иметь уже предобученные веса на базе изображений ImageNet. В качестве политики изменения темпа обучения была использована политика экспоненциального затухания с оптимальными параметрами initial_lrate = 0.01 и k = 0.3 (initial_lrate - начальный темп обучения, k - коэффициент наклона экспоненциальной кривой).  
```
def build_model():  
  inputs = tf.keras.Input(shape=(RESIZE_TO, RESIZE_TO, 3))  
  x = tf.keras.layers.GaussianNoise(stddev = 0.6)(inputs)  
  x = tf.keras.layers.experimental.preprocessing.RandomCrop(224,224)(x)  
  x = tf.keras.layers.experimental.preprocessing.RandomRotation(factor = 0.02)(x)  
  model = EfficientNetB0(include_top=False, input_tensor = x, pooling = 'avg', weights='imagenet')  
  model.trainable = False  
  x = tf.keras.layers.Flatten()(model.output)  
  outputs = tf.keras.layers.Dense(NUM_CLASSES, activation = tf.keras.activations.softmax)(x)  
  return tf.keras.Model(inputs=inputs, outputs=outputs)  
```
Обучение нейронной сети будет проводиться в два этапа:  
+ Первый этап - заморозить все слои нейронной сети EfficientNet-B0 (model.trainable = False) и обучить только верхние слои (классификатор). - Transfer Learning    
+ Второй этоп - разморозить несколько верхних слоев сети EfficientNet-B0, а затем дообучить ее с меньшим темпом обучения. - Fine Tuning   
Таким образом нейронная сеть будет лучше выделять какие-то характерные признаки тех объектов, которые входят в используемый датасет Oregon Wildlife.  
Функция для разморозки верхних слоев нейронной сети EfficientNet-B0:  
```
def unfreeze_model(model):  
  for layer in model.layers[-20:]:  
    if not isinstance(layer, tf.keras.layers.BatchNormalization):  
      layer.trainable = True  
```
С помощью данной функции будут размораживаться последние 20 слоев нейронной сети EfficientNet-B0, кроме слоев ```BatchNormalization```.  
## С использованием техники обучения Transfer Learning и оптимальной политики изменения темпа обучения обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Oregon Wildlife с совместным использованием техник аугментации данных с оптимальными параметрами:  
#### В результате получили следующие графики:  
+ График метрики точности для предобученной нейронной сети EfficientNet-B0 для валидационного набора данных:

![legend_accuracy_brigthness_contrast](https://user-images.githubusercontent.com/59259102/112815848-6b4c1680-9089-11eb-9ef2-a06840cf72f3.png)  

<img src="./epoch_categorical_accuracy_brigtness_contrast.svg">

* График функции потерь для предобученной нейронной сети EfficientNet-B0 для валидационного набора данных:  

![legend_loss_brightness_contrast](https://user-images.githubusercontent.com/59259102/112822862-29bf6980-9091-11eb-8989-d7d83ce88fc8.png)  
 

<img src="./epoch_loss_brightness_contrast.svg">


* Анализ полученных результатов  
Исходя из полученных результатов в качестве оптимальных параметров можно использовать: contrast_factor = 2, delta = 0.3 (contrast_factor - множитель для регулировки контраста, delta - число добавляемое к значениям пикселей для изменения яркости).
  + По сравнению с другими использовавшимися комбинациями:
    1. Одна из наибыстрейших скоростей схождения алгоритма (лучше только у комбинаций contrast_factor = 1, delta = 0.1 и contrast_factor = 3, delta = 0.1, но при таких комбинациях потерь больше, и точность меньше, а скорость выше только на 1 эпоху)  
    2. Одна из наибольших метрик точности в конце обучения (лучше только у комбинации contrast_factor = 2, delta = 0.1, но при такой комбинации потерь больше, и скорость схождения алгоритма ниже, а точность больше только на 0.02 процента)  
    3. Наименьшее значение потерь в конце обучения (значение потерь 0.2798)  
  + По сравнению с политикой без аугментации данных:
    1. Увеличилась скорость схождения алгоритма, по сравнению с политикой без аугментации данных (увеличение скорости на 6 эпох)  
    2. Увеличение точности в конце обучения, по сравнению с политикой без аугментации данных (увеличение точности на 0.76 процента)  
    3. Не удалось уменьшить значение потерь в конце обучения, по сравнению с политикой без аугментации данных (увеличение потерь на 0.0145) 
***
Таким образом, использование техники манипуляции с яркостью и контрастом  привело к улучшению сходимости алгоритма обучения и по скорости, и по точности, но не по значению потерь при оптимально подобранных параметрах, в сравнении с политикой без аугментации данных. 
